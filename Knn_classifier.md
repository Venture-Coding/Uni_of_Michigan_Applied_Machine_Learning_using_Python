KNN - K- nearest neighbour 


<img width="694" alt="Screenshot 2021-11-11 at 3 22 18 PM" src="https://user-images.githubusercontent.com/61674750/141277123-e86da2f9-dd76-40b1-9cc7-7ff68caa3b8a.png">


Notes:

 *1. Under "minkowski" metric, when p=2, its Euclidean, p=1 gives Manhattan and p=infinity gives Chebychev's metric.
 *2. KNN Classifier is more generally known as an estimator.
 *3. The k-nearest neighbors classification algorithm has to memorize all of the training examples to make a prediction.
 *4. Setting “k” to the number of points in the training set will result in a classifier that always predicts the majority class.
 *5. A low value of “k” (close to 1) is more likely to overfit the training data and lead to worse accuracy on the test data, compared to higher values of “k”.
